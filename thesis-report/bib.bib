@article{Vaswani2017,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
archivePrefix = {arXiv},
arxivId = {1706.03762},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
eprint = {1706.03762},
file = {::},
month = {jun},
title = {{Attention Is All You Need}},
url = {http://arxiv.org/abs/1706.03762},
year = {2017}
}
@inproceedings{Stern2017,
abstract = {In this work, we present a minimal neural model for constituency parsing based on independent scoring of labels and spans. We show that this model is not only compatible with classical dynamic programming techniques, but also admits a novel greedy top-down inference algorithm based on recursive partitioning of the input. We demonstrate empirically that both prediction schemes are competitive with recent work, and when combined with basic extensions to the scoring model are capable of achieving state-of-the-art single-model performance on the Penn Treebank (91.79 F1) and strong performance on the French Treebank (82.23 F1).},
address = {Stroudsburg, PA, USA},
author = {Stern, Mitchell and Andreas, Jacob and Klein, Dan},
booktitle = {Proceedings of the 55th Annual Meeting of the Association for
          Computational Linguistics (Volume 1: Long Papers)},
doi = {10.18653/v1/P17-1076},
file = {::},
pages = {818--827},
publisher = {Association for Computational Linguistics},
title = {{A Minimal Span-Based Neural Constituency Parser}},
url = {http://aclweb.org/anthology/P17-1076},
year = {2017}
}
@inproceedings{Gaddy2018,
abstract = {A number of differences have emerged between modern and classic approaches to constituency parsing in recent years, with structural components like grammars and feature-rich lexicons becoming less central while recurrent neural network representations rise in popularity. The goal of this work is to analyze the extent to which information provided directly by the model structure in classical systems is still being captured by neural methods. To this end, we propose a high-performance neural model (92.08 F1 on PTB) that is representative of recent work and perform a series of investigative experiments. We find that our model implicitly learns to encode much of the same information that was explicitly provided by grammars and lexicons in the past, indicating that this scaffolding can largely be subsumed by powerful general-purpose neural machinery.},
address = {Stroudsburg, PA, USA},
author = {Gaddy, David and Stern, Mitchell and Klein, Dan},
booktitle = {Proceedings of the 2018 Conference of the North American Chapter of
          the Association for Computational Linguistics: Human Language
          Technologies, Volume 1 (Long Papers)},
doi = {10.18653/v1/N18-1091},
file = {::},
pages = {999--1010},
publisher = {Association for Computational Linguistics},
title = {{What's Going On in Neural Constituency Parsers? An Analysis}},
url = {http://aclweb.org/anthology/N18-1091},
year = {2018}
}
@inproceedings{Kitaev2019,
abstract = {We demonstrate that replacing an LSTM encoder with a self-attentive architecture can lead to improvements to a state-of-the-art discriminative constituency parser. The use of attention makes explicit the manner in which information is propagated between different locations in the sentence, which we use to both analyze our model and propose potential improvements. For example, we find that separating positional and content information in the encoder can lead to improved parsing accuracy. Additionally, we evaluate different approaches for lexical representation. Our parser achieves new state-of-the-art results for single models trained on the Penn Treebank: 93.55 F1 without the use of any external data, and 95.13 F1 when using pre-trained word representations. Our parser also outperforms the previous best-published accuracy figures on 8 of the 9 languages in the SPMRL dataset.},
author = {Kitaev, Nikita and Klein, Dan},
doi = {10.18653/v1/p18-1249},
file = {::},
month = {jun},
pages = {2676--2686},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Constituency Parsing with a Self-Attentive Encoder}},
year = {2019}
}
@inproceedings{RichardSocher2011,
abstract = {Recursive structure is commonly found in the inputs of different modalities such as natural scene images or natural language sentences. Discovering this recursive structure helps us to not only identify the units that an image or sentence contains but also how they interact to form a whole. We introduce a max-margin structure prediction architecture based on re-cursive neural networks that can successfully recover such structure both in complex scene images as well as sentences. The same algo-rithm can be used both to provide a competi-tive syntactic parser for natural language sen-tences from the Penn Treebank and to out-perform alternative approaches for semantic scene segmentation, annotation and classifi-cation. For segmentation and annotation our algorithm obtains a new level of state-of-the-art performance on the Stanford background dataset (78.1{\%}). The features from the im-age parse tree outperform Gist descriptors for scene classification by 4{\%}.},
archivePrefix = {arXiv},
arxivId = {arXiv:1207.6324},
author = {{Richard Socher}},
doi = {10.1007/s10107-018-1337-6},
eprint = {arXiv:1207.6324},
file = {::},
isbn = {9781450306195},
pmid = {22183238},
title = {{Parsing Natural scenes and natural language with recursive neural networks}},
year = {2011}
}
@techreport{Socher,
abstract = {Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparse-ness. Instead, we introduce a Compo-sitional Vector Grammar (CVG), which combines PCFGs with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations. The CVG improves the PCFG of the Stanford Parser by 3.8{\%} to obtain an F1 score of 90.4{\%}. It is fast to train and implemented approximately as an efficient reranker it is about 20{\%} faster than the current Stanford factored parser. The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments.},
author = {Socher, Richard and Bauer, John and Manning, Christopher D and Ng, Andrew Y},
file = {::},
pages = {455--465},
title = {{Parsing with Compositional Vector Grammars}}
}
@techreport{Sochera,
abstract = {Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lex-icalizing phrases only partly address the problem at the cost of huge feature spaces and sparseness. To address this, we introduce a recursive neural network architecture for jointly parsing natural language and learning vector space representations for variable-sized inputs. At the core of our architecture are context-aware re-cursive neural networks (CRNN). These networks can induce distributed feature representations for unseen phrases and provide syntactic information to accurately predict phrase structure trees. Most excitingly, the representation of each phrase also captures semantic information: For instance, the phrases "decline to com-ment" and "would not disclose the terms" are close by in the induced embedding space. Our current system achieves an unlabeled bracketing F-measure of 92.1{\%} on the Wall Street Journal development dataset for sentences up to length 15.},
author = {Socher, Richard and Manning, Christopher D and Ng, Andrew Y},
file = {::},
title = {{Learning Continuous Phrase Representations and Syntactic Parsing with Recursive Neural Networks}}
}
@article{Jurafsky2008,
author = {Jurafsky, Daniel and Martin, James H},
file = {:media/kandy/hdd/master-thesis/constituency-parsing/literature/Daniel{\_}Jurafsky,{\_}James{\_}Martin-sep2018-draft-Speech{\_}and{\_}Language{\_}Processing.pdf:pdf},
journal = {Computational Linguistics and Natural Language Processing.2nd Edn., Prentice Hall, ISBN},
number = {0131873210},
pages = {794--800},
title = {{Speech and language processing: An introduction to speech recognition}},
volume = {10},
year = {2008}
}

@inproceedings{Gildea:2002:NPP:1073083.1073124,
 author = {Gildea, Daniel and Palmer, Martha},
 title = {The Necessity of Parsing for Predicate Argument Recognition},
 booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
 series = {ACL '02},
 year = {2002},
 location = {Philadelphia, Pennsylvania},
 pages = {239--246},
 numpages = {8},
 url = {https://doi.org/10.3115/1073083.1073124},
 doi = {10.3115/1073083.1073124},
 acmid = {1073124},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
} 


@article{Callison-Burch2010,
abstract = {We improve the quality of paraphrases ex-tracted from parallel corpora by requiring that phrases and their paraphrases be the same syn-tactic type. This is achieved by parsing the En-glish side of a parallel corpus and altering the phrase extraction algorithm to extract phrase labels alongside bilingual phrase pairs. In or-der to retain broad coverage of non-constituent phrases, complex syntactic labels are intro-duced. A manual evaluation indicates a 19{\%} absolute improvement in paraphrase quality over the baseline method.},
author = {Callison-Burch, Chris},
doi = {10.3115/1613715.1613743},
file = {:media/kandy/hdd/master-thesis/constituency-parsing/literature/application/D08-1021.pdf:pdf},
number = {October},
pages = {196},
title = {{Syntactic constraints on paraphrases extracted from parallel corpora}},
year = {2010}
}

@article{Marcus1993,
author = {Marcus, Mitchell P},
file = {:media/kandy/hdd/master-thesis/constituency-parsing/literature/others/J93-2004.pdf:pdf},
title = {{J93-2004.pdf}},
year = {1993}
}

@article{Taylor2003,
abstract = {The Penn Treebank, in its eight years of operation (1989-1996), produced approximately 7 million words of part-of-speech tagged text, 3 million words of skeletally parsed text, over 2 million words of text parsed for predicate-argument structure, and 1.6 million words of transcribed spoken text annotated for speech disfluencies. This paper describes the design of the three annotation schemes used by the Treebank: POS tagging, syntactic bracketing, and disfluency annotation and the methodology employed in production. All available Penn Treebank materials are distributed by the Linguistic Data Consortium http://www.ldc.upenn.edu.},
author = {Taylor, Ann and Marcus, Mitchell and Santorini, Beatrice},
doi = {10.1007/978-94-010-0201-1_1},
file = {:media/kandy/hdd/master-thesis/constituency-parsing/literature/others/10.1.1.9.8216.pdf:pdf},
pages = {5--22},
title = {{The Penn Treebank: An Overview}},
year = {2003}
}
