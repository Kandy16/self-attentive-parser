@article{Vaswani2017,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
archivePrefix = {arXiv},
arxivId = {1706.03762},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
eprint = {1706.03762},
file = {::},
month = {jun},
title = {{Attention Is All You Need}},
url = {http://arxiv.org/abs/1706.03762},
year = {2017}
}
@inproceedings{Stern2017,
abstract = {In this work, we present a minimal neural model for constituency parsing based on independent scoring of labels and spans. We show that this model is not only compatible with classical dynamic programming techniques, but also admits a novel greedy top-down inference algorithm based on recursive partitioning of the input. We demonstrate empirically that both prediction schemes are competitive with recent work, and when combined with basic extensions to the scoring model are capable of achieving state-of-the-art single-model performance on the Penn Treebank (91.79 F1) and strong performance on the French Treebank (82.23 F1).},
address = {Stroudsburg, PA, USA},
author = {Stern, Mitchell and Andreas, Jacob and Klein, Dan},
booktitle = {Proceedings of the 55th Annual Meeting of the Association for
          Computational Linguistics (Volume 1: Long Papers)},
doi = {10.18653/v1/P17-1076},
file = {::},
pages = {818--827},
publisher = {Association for Computational Linguistics},
title = {{A Minimal Span-Based Neural Constituency Parser}},
url = {http://aclweb.org/anthology/P17-1076},
year = {2017}
}
@inproceedings{Gaddy2018,
abstract = {A number of differences have emerged between modern and classic approaches to constituency parsing in recent years, with structural components like grammars and feature-rich lexicons becoming less central while recurrent neural network representations rise in popularity. The goal of this work is to analyze the extent to which information provided directly by the model structure in classical systems is still being captured by neural methods. To this end, we propose a high-performance neural model (92.08 F1 on PTB) that is representative of recent work and perform a series of investigative experiments. We find that our model implicitly learns to encode much of the same information that was explicitly provided by grammars and lexicons in the past, indicating that this scaffolding can largely be subsumed by powerful general-purpose neural machinery.},
address = {Stroudsburg, PA, USA},
author = {Gaddy, David and Stern, Mitchell and Klein, Dan},
booktitle = {Proceedings of the 2018 Conference of the North American Chapter of
          the Association for Computational Linguistics: Human Language
          Technologies, Volume 1 (Long Papers)},
doi = {10.18653/v1/N18-1091},
file = {::},
pages = {999--1010},
publisher = {Association for Computational Linguistics},
title = {{What's Going On in Neural Constituency Parsers? An Analysis}},
url = {http://aclweb.org/anthology/N18-1091},
year = {2018}
}
@inproceedings{Kitaev2019,
abstract = {We demonstrate that replacing an LSTM encoder with a self-attentive architecture can lead to improvements to a state-of-the-art discriminative constituency parser. The use of attention makes explicit the manner in which information is propagated between different locations in the sentence, which we use to both analyze our model and propose potential improvements. For example, we find that separating positional and content information in the encoder can lead to improved parsing accuracy. Additionally, we evaluate different approaches for lexical representation. Our parser achieves new state-of-the-art results for single models trained on the Penn Treebank: 93.55 F1 without the use of any external data, and 95.13 F1 when using pre-trained word representations. Our parser also outperforms the previous best-published accuracy figures on 8 of the 9 languages in the SPMRL dataset.},
author = {Kitaev, Nikita and Klein, Dan},
doi = {10.18653/v1/p18-1249},
file = {::},
month = {jun},
pages = {2676--2686},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Constituency Parsing with a Self-Attentive Encoder}},
year = {2019}
}
@inproceedings{RichardSocher2011,
abstract = {Recursive structure is commonly found in the inputs of different modalities such as natural scene images or natural language sentences. Discovering this recursive structure helps us to not only identify the units that an image or sentence contains but also how they interact to form a whole. We introduce a max-margin structure prediction architecture based on re-cursive neural networks that can successfully recover such structure both in complex scene images as well as sentences. The same algo-rithm can be used both to provide a competi-tive syntactic parser for natural language sen-tences from the Penn Treebank and to out-perform alternative approaches for semantic scene segmentation, annotation and classifi-cation. For segmentation and annotation our algorithm obtains a new level of state-of-the-art performance on the Stanford background dataset (78.1{\%}). The features from the im-age parse tree outperform Gist descriptors for scene classification by 4{\%}.},
archivePrefix = {arXiv},
arxivId = {arXiv:1207.6324},
author = {{Richard Socher}},
doi = {10.1007/s10107-018-1337-6},
eprint = {arXiv:1207.6324},
file = {::},
isbn = {9781450306195},
pmid = {22183238},
title = {{Parsing Natural scenes and natural language with recursive neural networks}},
year = {2011}
}
@techreport{Socher,
abstract = {Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparse-ness. Instead, we introduce a Compo-sitional Vector Grammar (CVG), which combines PCFGs with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations. The CVG improves the PCFG of the Stanford Parser by 3.8{\%} to obtain an F1 score of 90.4{\%}. It is fast to train and implemented approximately as an efficient reranker it is about 20{\%} faster than the current Stanford factored parser. The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments.},
author = {Socher, Richard and Bauer, John and Manning, Christopher D and Ng, Andrew Y},
file = {::},
pages = {455--465},
title = {{Parsing with Compositional Vector Grammars}}
}
@techreport{Sochera,
abstract = {Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lex-icalizing phrases only partly address the problem at the cost of huge feature spaces and sparseness. To address this, we introduce a recursive neural network architecture for jointly parsing natural language and learning vector space representations for variable-sized inputs. At the core of our architecture are context-aware re-cursive neural networks (CRNN). These networks can induce distributed feature representations for unseen phrases and provide syntactic information to accurately predict phrase structure trees. Most excitingly, the representation of each phrase also captures semantic information: For instance, the phrases "decline to com-ment" and "would not disclose the terms" are close by in the induced embedding space. Our current system achieves an unlabeled bracketing F-measure of 92.1{\%} on the Wall Street Journal development dataset for sentences up to length 15.},
author = {Socher, Richard and Manning, Christopher D and Ng, Andrew Y},
file = {::},
title = {{Learning Continuous Phrase Representations and Syntactic Parsing with Recursive Neural Networks}}
}
@article{Jurafsky2008,
author = {Jurafsky, Daniel and Martin, James H},
file = {:media/kandy/hdd/master-thesis/constituency-parsing/literature/Daniel{\_}Jurafsky,{\_}James{\_}Martin-sep2018-draft-Speech{\_}and{\_}Language{\_}Processing.pdf:pdf},
journal = {Computational Linguistics and Natural Language Processing.2nd Edn., Prentice Hall, ISBN},
number = {0131873210},
pages = {794--800},
title = {{Speech and language processing: An introduction to speech recognition}},
volume = {10},
year = {2008}
}

@inproceedings{Gildea:2002:NPP:1073083.1073124,
 author = {Gildea, Daniel and Palmer, Martha},
 title = {The Necessity of Parsing for Predicate Argument Recognition},
 booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
 series = {ACL '02},
 year = {2002},
 location = {Philadelphia, Pennsylvania},
 pages = {239--246},
 numpages = {8},
 url = {https://doi.org/10.3115/1073083.1073124},
 doi = {10.3115/1073083.1073124},
 acmid = {1073124},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
} 


@article{Callison-Burch2010,
abstract = {We improve the quality of paraphrases ex-tracted from parallel corpora by requiring that phrases and their paraphrases be the same syn-tactic type. This is achieved by parsing the En-glish side of a parallel corpus and altering the phrase extraction algorithm to extract phrase labels alongside bilingual phrase pairs. In or-der to retain broad coverage of non-constituent phrases, complex syntactic labels are intro-duced. A manual evaluation indicates a 19{\%} absolute improvement in paraphrase quality over the baseline method.},
author = {Callison-Burch, Chris},
doi = {10.3115/1613715.1613743},
file = {:media/kandy/hdd/master-thesis/constituency-parsing/literature/application/D08-1021.pdf:pdf},
number = {October},
pages = {196},
title = {{Syntactic constraints on paraphrases extracted from parallel corpora}},
year = {2010}
}

@article{Marcus1993,
author = {Marcus, Mitchell P},
file = {:media/kandy/hdd/master-thesis/constituency-parsing/literature/others/J93-2004.pdf:pdf},
title = {{J93-2004.pdf}},
year = {1993}
}

@article{Taylor2003,
abstract = {The Penn Treebank, in its eight years of operation (1989-1996), produced approximately 7 million words of part-of-speech tagged text, 3 million words of skeletally parsed text, over 2 million words of text parsed for predicate-argument structure, and 1.6 million words of transcribed spoken text annotated for speech disfluencies. This paper describes the design of the three annotation schemes used by the Treebank: POS tagging, syntactic bracketing, and disfluency annotation and the methodology employed in production. All available Penn Treebank materials are distributed by the Linguistic Data Consortium http://www.ldc.upenn.edu.},
author = {Taylor, Ann and Marcus, Mitchell and Santorini, Beatrice},
doi = {10.1007/978-94-010-0201-1_1},
file = {:media/kandy/hdd/master-thesis/constituency-parsing/literature/others/10.1.1.9.8216.pdf:pdf},
pages = {5--22},
title = {{The Penn Treebank: An Overview}},
year = {2003}
}

@article{GerElmo,
title = "{German ELMo Model}",
author = {Philip May},
year = "2019",
url = {https://github.com/t-systems-on-site-services-gmbh/german-elmo-model},
}

@incollection{NIPS2017_7181,
title = {Attention is All you Need},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5998--6008},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}
}

@misc{devlin2018bert,
    title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
    author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
    year={2018},
    eprint={1810.04805},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{YOUNGER1967189,
title = "Recognition and parsing of context-free languages in time n3",
journal = "Information and Control",
volume = "10",
number = "2",
pages = "189 - 208",
year = "1967",
issn = "0019-9958",
doi = "https://doi.org/10.1016/S0019-9958(67)80007-X",
url = "http://www.sciencedirect.com/science/article/pii/S001999586780007X",
author = "Daniel H. Younger",
abstract = "A recognition algorithm is exhibited whereby an arbitrary string over a given vocabulary can be tested for containment in a given context-free language. A special merit of this algorithm is that it is completed in a number of steps proportional to the “cube” of the number of symbols in the tested string. As a byproduct of the grammatical analysis, required by the recognition algorithm, one can obtain, by some additional processing not exceeding the “cube” factor of computational complexity, a parsing matrix—a complete summary of the grammatical structure of the sentence. It is also shown how, by means of a minor modification of the recognition algorithm, one can obtain an integer representing the ambiguity of the sentence, i.e., the number of distinct ways in which that sentence can be generated by the grammar. The recognition algorithm is then simulated on a Turing Machine. It is shown that this simulation likewise requires a number of steps proportional to only the “cube” of the test string length."
}


@misc{cnn-1,
 author  = "Alphex34",
 title   = "Convolutional neural network architecture",
 year    = "2015",
 url     = "https://en.wikipedia.org/wiki/Convolutional_neural_network#/media/File:Typical_cnn.png",
 note    = "By Aphex34 - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=45679374",
}

@misc{rnn-1,
 author  = "François Deloche",
 title   = "Recurrent neural network architecture",
 year    = "2017",
 url     = "https://commons.wikimedia.org/wiki/File:Typical_cnn.png",
 note    = "CC BY-SA 4.0",
}

@misc{rnn-2,
 author  = "François Deloche",
 title   = "LSTM architecture",
 year    = "2017",
 url     = "https://en.wikipedia.org/wiki/Recurrent_neural_network#/media/File:Long_Short-Term_Memory.svg",
 note    = "CC BY-SA 4.0",
}

@inproceedings  { lecun-bengio-95a,
original =    "orig/lecun-bengio-95a.ps.gz",
author  =       "LeCun, Y. and Bengio, Y.",
title   =       "Convolutional Networks for Images, Speech, and Time-Series",
booktitle=      "The Handbook of Brain Theory and Neural Networks",
year    =       "1995",
editor  =       "Arbib, M. A.",
publisher=      "MIT Press",
}

@article{Zhang2015,
abstract = {This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several largescale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.},
archivePrefix = {arXiv},
arxivId = {1509.01626},
author = {Zhang, Xiang and Zhao, Junbo and Lecun, Yann},
eprint = {1509.01626},
file = {:home/kandy/Desktop/papers/1509.01626.pdf:pdf},
isbn = {0123456789},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {neural-networks},
pages = {649--657},
title = {{Character-level convolutional networks for text classification}},
volume = {2015-January},
year = {2015}
}

@misc{cho2014learning,
    title={Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation},
    author={Kyunghyun Cho and Bart van Merrienboer and Caglar Gulcehre and Dzmitry Bahdanau and Fethi Bougares and Holger Schwenk and Yoshua Bengio},
    year={2014},
    eprint={1406.1078},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}


@article{Vaswani2017,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
archivePrefix = {arXiv},
arxivId = {1706.03762},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
eprint = {1706.03762},
month = {jun},
title = {{Attention Is All You Need}},
url = {http://arxiv.org/abs/1706.03762},
year = {2017}
}

@misc{bahdanau2014neural,
    title={Neural Machine Translation by Jointly Learning to Align and Translate},
    author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
    year={2014},
    eprint={1409.0473},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{peters2018deep,
    title={Deep contextualized word representations},
    author={Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
    year={2018},
    eprint={1802.05365},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{mikolov2013linguistic,
  added-at = {2016-04-04T12:53:11.000+0200},
  author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
  biburl = {https://www.bibsonomy.org/bibtex/2b7a129bf7263d5b98caedcf4cb6f43a1/thoni},
  booktitle = {HLT-NAACL},
  interhash = {14a567771d9ce43df2e38d88cbfc251f},
  intrahash = {b7a129bf7263d5b98caedcf4cb6f43a1},
  keywords = {deeplearning linguistic regularities thema thema:word_embeddings},
  pages = {746--751},
  timestamp = {2016-09-06T08:23:07.000+0200},
  title = {Linguistic Regularities in Continuous Space Word Representations.},
  year = 2013
}


@INPROCEEDINGS{4569603,
author={T. L. {Booth}},
booktitle={10th Annual Symposium on Switching and Automata Theory (swat 1969)},
title={Probabilistic representation of formal languages},
year={1969},
volume={},
number={},
pages={74-81},
keywords={Formal languages;Production systems;Probability;Computer science;Digital systems;Bridges;Statistical analysis},
doi={10.1109/SWAT.1969.17},
ISSN={0272-4847},
month={Oct},}

@inproceedings{kleinunlexical,
author = {Klein, Dan and Manning, Christopher D.},
title = {Accurate Unlexicalized Parsing},
year = {2003},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1075096.1075150},
doi = {10.3115/1075096.1075150},
booktitle = {Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1},
pages = {423–430},
numpages = {8},
location = {Sapporo, Japan},
series = {ACL ’03}
}

@inproceedings{petrov2006,
author = {Petrov, Slav and Barrett, Leon and Thibaux, Romain and Klein, Dan},
title = {Learning Accurate, Compact, and Interpretable Tree Annotation},
year = {2006},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1220175.1220230},
doi = {10.3115/1220175.1220230},
booktitle = {Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics},
pages = {433–440},
numpages = {8},
location = {Sydney, Australia},
series = {ACL-44}
}
  

@ARTICLE{schuster1997,
author={M. {Schuster} and K. K. {Paliwal}},
journal={IEEE Transactions on Signal Processing},
title={Bidirectional recurrent neural networks},
year={1997},
volume={45},
number={11},
pages={2673-2681},
keywords={speech recognition;pattern classification;statistical analysis;recurrent neural nets;learning by example;speech processing;bidirectional recurrent neural networks;training;regular recurrent neural network;negative time direction;positive time direction;classification experiments;regression experiments;artificial data;phonemes;TIMIT database;conditional posterior probability;complete symbol sequences;real data;speech recognition;learning from examples;Recurrent neural networks;Artificial neural networks;Training data;Databases;Probability;Shape;Parameter estimation;Speech recognition;Control systems;Telecommunication control},
doi={10.1109/78.650093},
ISSN={1941-0476},
month={Nov},}

@inproceedings{charniak2000,
author = {Charniak, Eugene},
title = {A Maximum-Entropy-Inspired Parser},
year = {2000},
publisher = {Association for Computational Linguistics},
address = {USA},
booktitle = {Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference},
pages = {132–139},
numpages = {8},
location = {Seattle, Washington},
series = {NAACL 2000}
}
  

@article{Charniak_1997, title={Statistical Techniques for Natural Language Parsing}, volume={18}, url={https://aaai.org/ojs/index.php/aimagazine/article/view/1320}, DOI={10.1609/aimag.v18i4.1320}, abstractNote={I review current statistical work on syntactic parsing and then consider part-of-speech tagging, which was the first syntactic problem to successfully be attacked by statistical techniques and also serves as a good warm-up for the main topic-statistical parsing. Here, I consider both the simplified case in which the input string is viewed as a string of parts of speech and the more interesting case in which the parser is guided by statistical information about the particular words in the sentence. Finally, I anticipate future research directions.}, number={4}, journal={AI Magazine}, author={Charniak, Eugene}, year={1997}, month={Dec.}, pages={33} }


