@article{Vaswani2017,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
archivePrefix = {arXiv},
arxivId = {1706.03762},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
eprint = {1706.03762},
file = {::},
month = {jun},
title = {{Attention Is All You Need}},
url = {http://arxiv.org/abs/1706.03762},
year = {2017}
}
@inproceedings{Stern2017,
abstract = {In this work, we present a minimal neural model for constituency parsing based on independent scoring of labels and spans. We show that this model is not only compatible with classical dynamic programming techniques, but also admits a novel greedy top-down inference algorithm based on recursive partitioning of the input. We demonstrate empirically that both prediction schemes are competitive with recent work, and when combined with basic extensions to the scoring model are capable of achieving state-of-the-art single-model performance on the Penn Treebank (91.79 F1) and strong performance on the French Treebank (82.23 F1).},
address = {Stroudsburg, PA, USA},
author = {Stern, Mitchell and Andreas, Jacob and Klein, Dan},
booktitle = {Proceedings of the 55th Annual Meeting of the Association for
          Computational Linguistics (Volume 1: Long Papers)},
doi = {10.18653/v1/P17-1076},
file = {::},
pages = {818--827},
publisher = {Association for Computational Linguistics},
title = {{A Minimal Span-Based Neural Constituency Parser}},
url = {http://aclweb.org/anthology/P17-1076},
year = {2017}
}
@inproceedings{Gaddy2018,
abstract = {A number of differences have emerged between modern and classic approaches to constituency parsing in recent years, with structural components like grammars and feature-rich lexicons becoming less central while recurrent neural network representations rise in popularity. The goal of this work is to analyze the extent to which information provided directly by the model structure in classical systems is still being captured by neural methods. To this end, we propose a high-performance neural model (92.08 F1 on PTB) that is representative of recent work and perform a series of investigative experiments. We find that our model implicitly learns to encode much of the same information that was explicitly provided by grammars and lexicons in the past, indicating that this scaffolding can largely be subsumed by powerful general-purpose neural machinery.},
address = {Stroudsburg, PA, USA},
author = {Gaddy, David and Stern, Mitchell and Klein, Dan},
booktitle = {Proceedings of the 2018 Conference of the North American Chapter of
          the Association for Computational Linguistics: Human Language
          Technologies, Volume 1 (Long Papers)},
doi = {10.18653/v1/N18-1091},
file = {::},
pages = {999--1010},
publisher = {Association for Computational Linguistics},
title = {{What's Going On in Neural Constituency Parsers? An Analysis}},
url = {http://aclweb.org/anthology/N18-1091},
year = {2018}
}
@inproceedings{Kitaev2019,
abstract = {We demonstrate that replacing an LSTM encoder with a self-attentive architecture can lead to improvements to a state-of-the-art discriminative constituency parser. The use of attention makes explicit the manner in which information is propagated between different locations in the sentence, which we use to both analyze our model and propose potential improvements. For example, we find that separating positional and content information in the encoder can lead to improved parsing accuracy. Additionally, we evaluate different approaches for lexical representation. Our parser achieves new state-of-the-art results for single models trained on the Penn Treebank: 93.55 F1 without the use of any external data, and 95.13 F1 when using pre-trained word representations. Our parser also outperforms the previous best-published accuracy figures on 8 of the 9 languages in the SPMRL dataset.},
author = {Kitaev, Nikita and Klein, Dan},
doi = {10.18653/v1/p18-1249},
file = {::},
month = {jun},
pages = {2676--2686},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Constituency Parsing with a Self-Attentive Encoder}},
year = {2019}
}
@inproceedings{RichardSocher2011,
abstract = {Recursive structure is commonly found in the inputs of different modalities such as natural scene images or natural language sentences. Discovering this recursive structure helps us to not only identify the units that an image or sentence contains but also how they interact to form a whole. We introduce a max-margin structure prediction architecture based on re-cursive neural networks that can successfully recover such structure both in complex scene images as well as sentences. The same algo-rithm can be used both to provide a competi-tive syntactic parser for natural language sen-tences from the Penn Treebank and to out-perform alternative approaches for semantic scene segmentation, annotation and classifi-cation. For segmentation and annotation our algorithm obtains a new level of state-of-the-art performance on the Stanford background dataset (78.1{\%}). The features from the im-age parse tree outperform Gist descriptors for scene classification by 4{\%}.},
archivePrefix = {arXiv},
arxivId = {arXiv:1207.6324},
author = {{Richard Socher}},
doi = {10.1007/s10107-018-1337-6},
eprint = {arXiv:1207.6324},
file = {::},
isbn = {9781450306195},
pmid = {22183238},
title = {{Parsing Natural scenes and natural language with recursive neural networks}},
year = {2011}
}
@techreport{Socher,
abstract = {Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparse-ness. Instead, we introduce a Compo-sitional Vector Grammar (CVG), which combines PCFGs with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations. The CVG improves the PCFG of the Stanford Parser by 3.8{\%} to obtain an F1 score of 90.4{\%}. It is fast to train and implemented approximately as an efficient reranker it is about 20{\%} faster than the current Stanford factored parser. The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments.},
author = {Socher, Richard and Bauer, John and Manning, Christopher D and Ng, Andrew Y},
file = {::},
pages = {455--465},
title = {{Parsing with Compositional Vector Grammars}}
}
@techreport{Sochera,
abstract = {Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lex-icalizing phrases only partly address the problem at the cost of huge feature spaces and sparseness. To address this, we introduce a recursive neural network architecture for jointly parsing natural language and learning vector space representations for variable-sized inputs. At the core of our architecture are context-aware re-cursive neural networks (CRNN). These networks can induce distributed feature representations for unseen phrases and provide syntactic information to accurately predict phrase structure trees. Most excitingly, the representation of each phrase also captures semantic information: For instance, the phrases "decline to com-ment" and "would not disclose the terms" are close by in the induced embedding space. Our current system achieves an unlabeled bracketing F-measure of 92.1{\%} on the Wall Street Journal development dataset for sentences up to length 15.},
author = {Socher, Richard and Manning, Christopher D and Ng, Andrew Y},
file = {::},
title = {{Learning Continuous Phrase Representations and Syntactic Parsing with Recursive Neural Networks}}
}
@article{Jurafsky2008,
author = {Jurafsky, Daniel and Martin, James H},
file = {:media/kandy/hdd/master-thesis/constituency-parsing/literature/Daniel{\_}Jurafsky,{\_}James{\_}Martin-sep2018-draft-Speech{\_}and{\_}Language{\_}Processing.pdf:pdf},
journal = {Computational Linguistics and Natural Language Processing.2nd Edn., Prentice Hall, ISBN},
number = {0131873210},
pages = {794--800},
title = {{Speech and language processing: An introduction to speech recognition}},
volume = {10},
year = {2008}
}

@inproceedings{Gildea:2002:NPP:1073083.1073124,
 author = {Gildea, Daniel and Palmer, Martha},
 title = {The Necessity of Parsing for Predicate Argument Recognition},
 booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
 series = {ACL '02},
 year = {2002},
 location = {Philadelphia, Pennsylvania},
 pages = {239--246},
 numpages = {8},
 url = {https://doi.org/10.3115/1073083.1073124},
 doi = {10.3115/1073083.1073124},
 acmid = {1073124},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
} 


@article{Callison-Burch2010,
abstract = {We improve the quality of paraphrases ex-tracted from parallel corpora by requiring that phrases and their paraphrases be the same syn-tactic type. This is achieved by parsing the En-glish side of a parallel corpus and altering the phrase extraction algorithm to extract phrase labels alongside bilingual phrase pairs. In or-der to retain broad coverage of non-constituent phrases, complex syntactic labels are intro-duced. A manual evaluation indicates a 19{\%} absolute improvement in paraphrase quality over the baseline method.},
author = {Callison-Burch, Chris},
doi = {10.3115/1613715.1613743},
file = {:media/kandy/hdd/master-thesis/constituency-parsing/literature/application/D08-1021.pdf:pdf},
number = {October},
pages = {196},
title = {{Syntactic constraints on paraphrases extracted from parallel corpora}},
year = {2010}
}

@article{Marcus1993,
author = {Marcus, Mitchell P},
file = {:media/kandy/hdd/master-thesis/constituency-parsing/literature/others/J93-2004.pdf:pdf},
title = {{J93-2004.pdf}},
year = {1993}
}

@article{Taylor2003,
abstract = {The Penn Treebank, in its eight years of operation (1989-1996), produced approximately 7 million words of part-of-speech tagged text, 3 million words of skeletally parsed text, over 2 million words of text parsed for predicate-argument structure, and 1.6 million words of transcribed spoken text annotated for speech disfluencies. This paper describes the design of the three annotation schemes used by the Treebank: POS tagging, syntactic bracketing, and disfluency annotation and the methodology employed in production. All available Penn Treebank materials are distributed by the Linguistic Data Consortium http://www.ldc.upenn.edu.},
author = {Taylor, Ann and Marcus, Mitchell and Santorini, Beatrice},
doi = {10.1007/978-94-010-0201-1_1},
file = {:media/kandy/hdd/master-thesis/constituency-parsing/literature/others/10.1.1.9.8216.pdf:pdf},
pages = {5--22},
title = {{The Penn Treebank: An Overview}},
year = {2003}
}

@article{GerElmo,
title = "{German ELMo Model}",
author = {Philip May},
year = "2019",
url = {https://github.com/t-systems-on-site-services-gmbh/german-elmo-model},
}

@incollection{NIPS2017_7181,
title = {Attention is All you Need},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5998--6008},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}
}

@misc{devlin2018bert,
    title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
    author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
    year={2018},
    eprint={1810.04805},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{YOUNGER1967189,
title = "Recognition and parsing of context-free languages in time n3",
journal = "Information and Control",
volume = "10",
number = "2",
pages = "189 - 208",
year = "1967",
issn = "0019-9958",
doi = "https://doi.org/10.1016/S0019-9958(67)80007-X",
url = "http://www.sciencedirect.com/science/article/pii/S001999586780007X",
author = "Daniel H. Younger",
abstract = "A recognition algorithm is exhibited whereby an arbitrary string over a given vocabulary can be tested for containment in a given context-free language. A special merit of this algorithm is that it is completed in a number of steps proportional to the “cube” of the number of symbols in the tested string. As a byproduct of the grammatical analysis, required by the recognition algorithm, one can obtain, by some additional processing not exceeding the “cube” factor of computational complexity, a parsing matrix—a complete summary of the grammatical structure of the sentence. It is also shown how, by means of a minor modification of the recognition algorithm, one can obtain an integer representing the ambiguity of the sentence, i.e., the number of distinct ways in which that sentence can be generated by the grammar. The recognition algorithm is then simulated on a Turing Machine. It is shown that this simulation likewise requires a number of steps proportional to only the “cube” of the test string length."
}


@misc{cnn-1,
 author  = "Alphex34",
 title   = "Convolutional neural network architecture",
 year    = "2015",
 url     = "https://en.wikipedia.org/wiki/Convolutional_neural_network#/media/File:Typical_cnn.png",
 note    = "By Aphex34 - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=45679374",
}

@misc{rnn-1,
 author  = "François Deloche",
 title   = "Recurrent neural network architecture",
 year    = "2017",
 url     = "https://commons.wikimedia.org/wiki/File:Typical_cnn.png",
 note    = "CC BY-SA 4.0",
}

@misc{rnn-2,
 author  = "François Deloche",
 title   = "LSTM architecture",
 year    = "2017",
 url     = "https://en.wikipedia.org/wiki/Recurrent_neural_network#/media/File:Long_Short-Term_Memory.svg",
 note    = "CC BY-SA 4.0",
}

@inproceedings  { lecun-bengio-95a,
original =    "orig/lecun-bengio-95a.ps.gz",
author  =       "LeCun, Y. and Bengio, Y.",
title   =       "Convolutional Networks for Images, Speech, and Time-Series",
booktitle=      "The Handbook of Brain Theory and Neural Networks",
year    =       "1995",
editor  =       "Arbib, M. A.",
publisher=      "MIT Press",
}

@article{Zhang2015,
abstract = {This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several largescale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.},
archivePrefix = {arXiv},
arxivId = {1509.01626},
author = {Zhang, Xiang and Zhao, Junbo and Lecun, Yann},
eprint = {1509.01626},
file = {:home/kandy/Desktop/papers/1509.01626.pdf:pdf},
isbn = {0123456789},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {neural-networks},
pages = {649--657},
title = {{Character-level convolutional networks for text classification}},
volume = {2015-January},
year = {2015}
}

@misc{cho2014learning,
    title={Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation},
    author={Kyunghyun Cho and Bart van Merrienboer and Caglar Gulcehre and Dzmitry Bahdanau and Fethi Bougares and Holger Schwenk and Yoshua Bengio},
    year={2014},
    eprint={1406.1078},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}


@article{Vaswani2017,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
archivePrefix = {arXiv},
arxivId = {1706.03762},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
eprint = {1706.03762},
month = {jun},
title = {{Attention Is All You Need}},
url = {http://arxiv.org/abs/1706.03762},
year = {2017}
}

@misc{bahdanau2014neural,
    title={Neural Machine Translation by Jointly Learning to Align and Translate},
    author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
    year={2014},
    eprint={1409.0473},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{peters2018deep,
    title={Deep contextualized word representations},
    author={Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
    year={2018},
    eprint={1802.05365},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{mikolov2013linguistic,
  added-at = {2016-04-04T12:53:11.000+0200},
  author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
  biburl = {https://www.bibsonomy.org/bibtex/2b7a129bf7263d5b98caedcf4cb6f43a1/thoni},
  booktitle = {HLT-NAACL},
  interhash = {14a567771d9ce43df2e38d88cbfc251f},
  intrahash = {b7a129bf7263d5b98caedcf4cb6f43a1},
  keywords = {deeplearning linguistic regularities thema thema:word_embeddings},
  pages = {746--751},
  timestamp = {2016-09-06T08:23:07.000+0200},
  title = {Linguistic Regularities in Continuous Space Word Representations.},
  year = 2013
}


@INPROCEEDINGS{4569603,
author={T. L. {Booth}},
booktitle={10th Annual Symposium on Switching and Automata Theory (swat 1969)},
title={Probabilistic representation of formal languages},
year={1969},
volume={},
number={},
pages={74-81},
keywords={Formal languages;Production systems;Probability;Computer science;Digital systems;Bridges;Statistical analysis},
doi={10.1109/SWAT.1969.17},
ISSN={0272-4847},
month={Oct},}

@inproceedings{kleinunlexical,
author = {Klein, Dan and Manning, Christopher D.},
title = {Accurate Unlexicalized Parsing},
year = {2003},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1075096.1075150},
doi = {10.3115/1075096.1075150},
booktitle = {Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1},
pages = {423–430},
numpages = {8},
location = {Sapporo, Japan},
series = {ACL ’03}
}

@inproceedings{petrov2006,
author = {Petrov, Slav and Barrett, Leon and Thibaux, Romain and Klein, Dan},
title = {Learning Accurate, Compact, and Interpretable Tree Annotation},
year = {2006},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1220175.1220230},
doi = {10.3115/1220175.1220230},
booktitle = {Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics},
pages = {433–440},
numpages = {8},
location = {Sydney, Australia},
series = {ACL-44}
}
  

@ARTICLE{schuster1997,
author={M. {Schuster} and K. K. {Paliwal}},
journal={IEEE Transactions on Signal Processing},
title={Bidirectional recurrent neural networks},
year={1997},
volume={45},
number={11},
pages={2673-2681},
keywords={speech recognition;pattern classification;statistical analysis;recurrent neural nets;learning by example;speech processing;bidirectional recurrent neural networks;training;regular recurrent neural network;negative time direction;positive time direction;classification experiments;regression experiments;artificial data;phonemes;TIMIT database;conditional posterior probability;complete symbol sequences;real data;speech recognition;learning from examples;Recurrent neural networks;Artificial neural networks;Training data;Databases;Probability;Shape;Parameter estimation;Speech recognition;Control systems;Telecommunication control},
doi={10.1109/78.650093},
ISSN={1941-0476},
month={Nov},}

@inproceedings{charniak2000,
author = {Charniak, Eugene},
title = {A Maximum-Entropy-Inspired Parser},
year = {2000},
publisher = {Association for Computational Linguistics},
address = {USA},
booktitle = {Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference},
pages = {132–139},
numpages = {8},
location = {Seattle, Washington},
series = {NAACL 2000}
}
  

@article{Charniak_1997, title={Statistical Techniques for Natural Language Parsing}, volume={18}, url={https://aaai.org/ojs/index.php/aimagazine/article/view/1320}, DOI={10.1609/aimag.v18i4.1320}, abstractNote={I review current statistical work on syntactic parsing and then consider part-of-speech tagging, which was the first syntactic problem to successfully be attacked by statistical techniques and also serves as a good warm-up for the main topic-statistical parsing. Here, I consider both the simplified case in which the input string is viewed as a string of parts of speech and the more interesting case in which the parser is guided by statistical information about the particular words in the sentence. Finally, I anticipate future research directions.}, number={4}, journal={AI Magazine}, author={Charniak, Eugene}, year={1997}, month={Dec.}, pages={33} }

@INPROCEEDINGS{Telljohann04thetuba,
    author = {Heike Telljohann and Erhard Hinrichs and Sandra Kübler and Ra Kübler and Universität Tübingen},
    title = {The Tüba-D/Z Treebank: Annotating German with a Context-Free Backbone},
    booktitle = {In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC 2004},
    year = {2004},
    pages = {2229--2235}
}

@article{TubingenTreebank,
author = {Telljohann, Heike and Hinrichs, Erhard},
year = {2004},
month = {03},
pages = {},
title = {Stylebook for the Tübingen Treebank of Written German (TüBa-D/Z)}
}

@inproceedings{Brants2002TheTT,
  title={The TIGER Treebank},
  author={Sabine Brants and Stefanie Dipper and Silvia Hansen and Wolfgang Lezius and George Barrett Smith},
  year={2002}
}

@article{Svozil1997,
abstract = {Basic definitions concerning the multi-layer feed-forward neural networks are given. The back-propagation training algo-rithm is explained. Partial derivatives of the objective function with respect to the weight and threshold coefficients are de-rived. These derivatives are valuable for an adaptation process of the considered neural network. Training and generalisation of multi-layer feed-forward neural networks are discussed. Improvements of the standard back-propagation algorithm are re-viewed. Example of the use of multi-layer feed-forward neural networks for prediction of carbon-13 NMR chemical shifts of alkanes is given. Further applications of neural networks in chemistry are reviewed. Advantages and disadvantages of multi-layer feed-forward neural networks are discussed. 0 1997 Elsevier Science B.V.},
author = {Svozil, Daniel and Kvasnieka, Vladimir and Pospichal, Jie},
file = {::},
journal = {Chemometrics and Intelligent Laboratory Systems},
keywords = {Back-propagation network Contents,Neural networks},
pages = {43--62},
title = {{Chemometrics and intelligent laboratory systems Introduction to multi-layer feed-forward neural networks}},
volume = {39},
year = {1997}
}

@article{Goldberg2016,
abstract = {Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.},
author = {Goldberg, Yoav},
file = {::},
journal = {Journal of Artificial Intelligence Research},
pages = {345--420},
title = {{A Primer on Neural Network Models for Natural Language Processing}},
volume = {57},
year = {2016}
}

@incollection{Bottou2012,
abstract = {Chapter 1 strongly advocates the stochastic back-propagation method to train neural networks. This is in fact an instance of a more general technique called stochastic gradient descent (SGD). This chapter provides background material, explains why SGD is a good learning algorithm when the training set is large, and provides useful recommendations.},
address = {Berlin, Heidelberg},
author = {Bottou, L{\'{e}}on},
booktitle = {Neural Networks: Tricks of the Trade: Second Edition},
doi = {10.1007/978-3-642-35289-8_25},
editor = {{Montavon Gr{\'{e}}goire
and Orr}, Genevi{\`{e}}ve B.
and M{\"{u}}ller Klaus-Robert},
isbn = {978-3-642-35289-8},
pages = {421--436},
publisher = {Springer Berlin Heidelberg},
title = {{Stochastic Gradient Descent Tricks}},
url = {https://doi.org/10.1007/978-3-642-35289-8{\_}25},
year = {2012}
}
@incollection{LeCunYannA.andBottou2012,
abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observed by practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposed in serious technical publications. This paper gives some of those tricks, and offers explanations of why they work.},
address = {Berlin, Heidelberg},
author = {{LeCun Yann A.
and Bottou}, L{\'{e}}on
and Orr Genevieve B.
and M{\"{u}}ller Klaus-Robert},
booktitle = {Neural Networks: Tricks of the Trade: Second Edition},
doi = {10.1007/978-3-642-35289-8_3},
editor = {{Montavon Gr{\'{e}}goire
and Orr}, Genevi{\`{e}}ve B.
and M{\"{u}}ller Klaus-Robert},
isbn = {978-3-642-35289-8},
pages = {9--48},
publisher = {Springer Berlin Heidelberg},
title = {{Efficient BackProp}},
url = {https://doi.org/10.1007/978-3-642-35289-8{\_}3},
year = {2012}
}

@article{Sutskever2013,
abstract = {Recently, pre-trained deep neural networks (DNNs) have outperformed traditional acoustic models based on Gaussian mixture models (GMMs) on a variety of large vocabulary speech recognition benchmarks. Deep neural nets have also achieved excellent results on various computer vision tasks using a random “dropout” procedure that drastically improves generalization error by randomly omitting a fraction of the hidden units in all layers. Since dropout helps avoid overﬁtting, it has also been successful on a small-scale phone recognition task using larger neural nets. However, training deep neural net acoustic models for large vocabulary speech recognition takes a very long time and dropout is likely to only increase training time. Neural networks with rectiﬁed linear unit (ReLU) non-linearities have been highly successful for computer vision tasks and proved faster to train than standard sigmoid units, sometimes also improving discriminative performance. In this work, we show on a 50-hour English Broadcast News task that modiﬁed deep neural networks using ReLUs trained with dropout during frame level training provide an 4.2{\%} relative improvement over a DNN trained with sigmoid units, and a 14.4{\%} relative improvement over a strong GMM/HMM system. We were able to obtain our results with minimal human hyper-parameter tuning using publicly available Bayesian optimization code},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3605v3},
author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
doi = {10.1109/ICASSP.2013.6639346},
eprint = {arXiv:1301.3605v3},
file = {:home/kandy/koblenz-web-science/master-thesis/papers/1{\_}new/sutskever13.pdf:pdf},
isbn = {9781479903566},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Bayesian optimization,LVCSR,acoustic modeling,broadcast news,deep learning,dropout,neural networks,rectified linear units},
number = {2010},
pages = {8609--8613},
pmid = {303902},
title = {{On the importance of initialization and momentum in deep learning}},
year = {2013}
}

@article{Duchi2011,
abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
archivePrefix = {arXiv},
arxivId = {arXiv:1103.4296v1},
author = {Duchi, John and Hazan, Elad and Singer, Yoram},
doi = {10.1109/CDC.2012.6426698},
eprint = {arXiv:1103.4296v1},
file = {:home/kandy/koblenz-web-science/master-thesis/papers/1{\_}new/duchi11a.pdf:pdf},
isbn = {9780982252925},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {adaptivity,online learning,stochastic convex optimization,subgradient methods},
pages = {2121--2159},
pmid = {2868127},
title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}},
url = {http://jmlr.org/papers/v12/duchi11a.html},
volume = {12},
year = {2011}
}

@article{Hinton2012,
abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
archivePrefix = {arXiv},
arxivId = {1207.0580},
author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
doi = {arXiv:1207.0580},
eprint = {1207.0580},
file = {::},
isbn = {9781467394673},
issn = {9781467394673},
pmid = {1000104337},
title = {{Improving neural networks by preventing co-adaptation of feature detectors}},
year = {2012}
}

@article{DBLP:journals/corr/abs-1301-3781,
  author    = {Tomas Mikolov and
               Kai Chen and
               Greg Corrado and
               Jeffrey Dean},
  title     = {Efficient Estimation of Word Representations in Vector Space},
  journal   = {CoRR},
  volume    = {abs/1301.3781},
  year      = {2013},
  url       = {http://arxiv.org/abs/1301.3781},
  archivePrefix = {arXiv},
  eprint    = {1301.3781},
  timestamp = {Wed, 07 Jun 2017 14:42:25 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1301-3781},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Bengio1997,
author = {Bengio, Y},
file = {::},
title = {{Convolutional Networks for Images, Speech, and Time-Series Parsing View project Oracle Performance for Visual Captioning View project}},
url = {https://www.researchgate.net/publication/2453996},
year = {1997}
}

@article{Elman,
abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to rep-resent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit pat-terns are fed back to themselves: the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simula-tions is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands: indeed, in this approach the notion of memory is inextri-cably bound up with task processing. These representations reveal a rich struc-ture, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.},
author = {Elman, Jeffrey L},
doi = {10.1207/s15516709cog1402_1},
file = {::},
journal = {COGNITIVE SCIENCE},
number = {1},
pages = {179--21},
title = {{Finding Structure in Time}},
volume = {14}
}

@article{Werbos1990,
abstract = {Basic backpropagation, which is a simple method now being widely$\backslash$nused in areas like pattern recognition and fault diagnosis, is reviewed.$\backslash$nThe basic equations for backpropagation through time, and applications$\backslash$nto areas like pattern recognition involving dynamic systems, systems$\backslash$nidentification, and control are discussed. Further extensions of this$\backslash$nmethod, to deal with systems other than neural networks, systems$\backslash$ninvolving simultaneous equations, or true recurrent networks, and other$\backslash$npractical issues arising with the method are described. Pseudocode is$\backslash$nprovided to clarify the algorithms. The chain rule for ordered$\backslash$nderivatives-the theorem which underlies backpropagation-is briefly$\backslash$ndiscussed. The focus is on designing a simpler version of$\backslash$nbackpropagation which can be translated into computer code and applied$\backslash$ndirectly by neutral network users},
author = {Werbos, Paul J.},
doi = {10.1109/5.58337},
file = {::},
isbn = {0018-9219},
issn = {15582256},
journal = {Proceedings of the IEEE},
title = {{Backpropagation Through Time: What It Does and How to Do It}},
year = {1990}
}

@article{Hochreiter1997,
abstract = { Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms. },
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
journal = {Neural Computation},
number = {8},
pages = {1735--1780},
title = {{Long Short-Term Memory}},
url = {https://doi.org/10.1162/neco.1997.9.8.1735},
volume = {9},
year = {1997}
}

@article{Parikh2016,
abstract = {We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.},
archivePrefix = {arXiv},
arxivId = {1606.01933},
author = {Parikh, Ankur P. and T{\"{a}}ckstr{\"{o}}m, Oscar and Das, Dipanjan and Uszkoreit, Jakob},
doi = {10.18653/v1/N16-1062},
eprint = {1606.01933},
file = {:home/kandy/koblenz-web-science/master-thesis/papers/attention-model/1606.01933.pdf:pdf},
isbn = {9781941643914},
issn = {0001-0782},
title = {{A Decomposable Attention Model for Natural Language Inference}},
url = {http://arxiv.org/abs/1606.01933},
year = {2016}
}
@article{Yang2016,
abstract = {We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the word and sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.},
archivePrefix = {arXiv},
arxivId = {1606.02393},
author = {Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
doi = {10.18653/v1/N16-1174},
eprint = {1606.02393},
file = {:home/kandy/koblenz-web-science/master-thesis/papers/attention-model/N16-1174.pdf:pdf},
isbn = {9781941643914},
issn = {1606.02393},
journal = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
pages = {1480--1489},
title = {{Hierarchical Attention Networks for Document Classification}},
url = {http://aclweb.org/anthology/N16-1174},
year = {2016}
}

@article{bidirectionalRNN,
author = {Schuster, M. and Paliwal, K.K.},
title = {Bidirectional Recurrent Neural Networks},
year = {1997},
issue_date = {November 1997},
publisher = {IEEE Press},
volume = {45},
number = {11},
issn = {1053-587X},
url = {https://doi.org/10.1109/78.650093},
doi = {10.1109/78.650093},
journal = {Trans. Sig. Proc.},
month = nov,
pages = {2673–2681},
numpages = {9}
}
